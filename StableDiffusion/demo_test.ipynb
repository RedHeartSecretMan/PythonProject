{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ***Stabled Diffusion***\n",
    "**参考内容**\n",
    " - [Stability Ai官网](https://stability.ai/) - [Github源码托管](https://github.com/CompVis/stable-diffusion) - [Hugging Face模型托管](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n",
    " - [DreamStudio Discord论坛](https://discord.com/channels/1002292111942635562/1010300608575709234) - [DreamStudio在线试用网站](https://beta.dreamstudio.ai/dream) \n",
    " - [Stable Diffusion技术细节](https://huggingface.co/blog/stable_diffusion) - [Stable Diffusion images and prompts仓库](https://lexica.art/)\n",
    " - [Open Ai官网](https://openai.com/) - [DALL·E2 Prompt](http://dallery.gallery/wp-content/uploads/2022/07/The-DALL%C2%B7E-2-prompt-book-v1.02.pdf)\n",
    "\n",
    "**模型结构**\n",
    "- *`文本编码器 CLIP`，将文本转换为 U-Net 可以理解的隐空间*\n",
    "- *`调度器`，用于在训练期间逐步向图像添加噪声*\n",
    "- *`运算核心 UNet`，由 ResNet 块组成，生成输入潜在表示，预测去噪图像的噪声残差*\n",
    "- *`变分自编码器 VAE`，将潜在表示解码为真实图像，训练期间编码器用于获取图像的潜在表示，推理过程使用解码器转换回图像*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一、**初始化运行环境**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import inspect\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    DDIMScheduler,\n",
    "    DDPMScheduler,\n",
    "    PNDMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    ")\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 二、**初始化调用函数**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 图像网格化\n",
    "def image_grid(inputs, x=1, y=1):\n",
    "    image_list = inputs\n",
    "    assert len(image_list) == x*y\n",
    "    width, height = image_list[0].size\n",
    "    grid = Image.new(\"RGB\", size=(x*width, y*height))\n",
    "    k = 1\n",
    "    for idx in range(x*y):\n",
    "        if x <= y:\n",
    "            if idx == k*y:\n",
    "                k += 1\n",
    "            grid.paste(image_list[idx], box=((idx-(k-1)*x)%y*width, idx//x*height))\n",
    "        if x > y:\n",
    "            grid.paste(image_list[idx], box=(idx%x*width, idx//x*height))\n",
    "    outputs = grid\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "# 展示图像\n",
    "def show_image(image_list, prompt=0, scale=5, dpi=300, colormap=None):\n",
    "    sizes = np.ceil(np.sqrt(np.array(len(image_list))))\n",
    "    plot.figure(num=prompt, figsize=(sizes*scale, sizes*scale), dpi=dpi)\n",
    "    for idx, image in enumerate(image_list):\n",
    "        plot.subplot(int(sizes), int(sizes), idx+1)\n",
    "        plot.imshow(image, cmap=colormap)\n",
    "        plot.axis(\"off\")\n",
    "    plot.show() \n",
    "    \n",
    "\n",
    "# 存储图像\n",
    "def save_image(image_list, save_path, prompt):\n",
    "    regex = r\"^[^/\\\\:\\*\\?\\\"\\'\\<\\>\\|]{1,255}\"\n",
    "    prompt = re.search(regex, prompt).group()\n",
    "    if not os.path.exists(os.path.join(save_path, prompt)):\n",
    "        os.makedirs(os.path.join(save_path, prompt))\n",
    "    for image in image_list:\n",
    "        fn_list = list(map(lambda string: os.path.splitext(string)[0], os.listdir(os.path.join(save_path, prompt))))\n",
    "        k = 0\n",
    "        for idx in range(len(fn_list)):\n",
    "            try:\n",
    "                fn_list[idx] = int(fn_list[idx])\n",
    "            except:\n",
    "                fn_list.remove(fn_list[idx-k])\n",
    "                k += 1\n",
    "        if len(fn_list) == 0:\n",
    "            image.save(os.path.join(save_path, prompt, f\"{str(0).zfill(4)}.png\"))\n",
    "        else:\n",
    "            name_index = (set(fn_list) ^ set(range(max(fn_list) + 1))).pop() if len(set(fn_list)) != max(\n",
    "                fn_list) + 1 else max(fn_list) + 1\n",
    "            image.save(os.path.join(save_path, prompt, f\"{str(name_index).zfill(4)}.png\"))\n",
    "\n",
    "\n",
    "# 文字加图像到图像的数据管道\n",
    "class StableDiffusionImgToImgPipeline(DiffusionPipeline):\n",
    "    def __init__(self, vae: AutoencoderKL,\n",
    "                 text_encoder: CLIPTextModel,\n",
    "                 tokenizer: CLIPTokenizer,\n",
    "                 unet: UNet2DConditionModel,\n",
    "                 scheduler: Union[DDIMScheduler,\n",
    "                                  DDPMScheduler,\n",
    "                                  PNDMScheduler,\n",
    "                                  LMSDiscreteScheduler,\n",
    "                                  EulerDiscreteScheduler,],\n",
    "                 safety_checker: StableDiffusionSafetyChecker,\n",
    "                 feature_extractor: CLIPFeatureExtractor):\n",
    "        super().__init__()\n",
    "        self.register_modules(vae=vae,\n",
    "                              text_encoder=text_encoder,\n",
    "                              tokenizer=tokenizer,\n",
    "                              unet=unet,\n",
    "                              scheduler=scheduler,\n",
    "                              safety_checker=safety_checker,\n",
    "                              feature_extractor=feature_extractor)\n",
    "    @staticmethod\n",
    "    def preprocess(inputs):\n",
    "        image = inputs\n",
    "        width, height = image.size\n",
    "        width, height = map(lambda x: x - x % 8, (width, height))\n",
    "        image = image.resize((width, height), resample=Image.Resampling.LANCZOS)\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        image = image[None].transpose(0, 3, 1, 2)\n",
    "        image = torch.from_numpy(image)\n",
    "        outputs = 2. * image - 1.\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, prompt: Union[str, List[str]],\n",
    "                       image: torch.FloatTensor,\n",
    "                       strength: float = 0.75,\n",
    "                       num_inference_steps: Optional[int] = 50,\n",
    "                       guidance_scale: Optional[float] = 7.5,\n",
    "                       eta: Optional[float] = 0.0,\n",
    "                       generator: Optional[torch.Generator] = None,\n",
    "                       output_type: Optional[str] = \"pil\"):\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            raise ValueError(f\"### \\\"prompt\\\" has to be of type str or list but is {type(prompt)}\")\n",
    "\n",
    "        if strength < 0 or strength > 1:\n",
    "            raise ValueError(f\"### The value of strength should in (0.0, 1.0) but is {strength}\")\n",
    "\n",
    "        # set timesteps\n",
    "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
    "        extra_set_kwargs = {}\n",
    "        offset = 0\n",
    "        if accepts_offset:\n",
    "            offset = 1\n",
    "            extra_set_kwargs[\"offset\"] = 1\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
    "\n",
    "        # encode the init image into latents and scale the latents\n",
    "        init_latents = self.vae.encode(self.preprocess(image).to(self.device)).latent_dist.sample()\n",
    "        init_latents = 0.18215 * init_latents\n",
    "\n",
    "        # prepare init_latents noise to latents\n",
    "        init_latents = torch.cat([init_latents] * batch_size)\n",
    "\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = int(num_inference_steps * strength) + offset\n",
    "        init_timestep = min(init_timestep, num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
    "        timesteps = torch.tensor([timesteps] * batch_size, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # add noise to latents using the timesteps\n",
    "        noise = torch.randn(init_latents.shape, generator=generator, device=self.device)\n",
    "        init_latents = self.scheduler.add_noise(init_latents, noise, timesteps)\n",
    "\n",
    "        # get prompt text embeddings\n",
    "        text_input = self.tokenizer(prompt,\n",
    "                                    padding=\"max_length\",\n",
    "                                    max_length=self.tokenizer.model_max_length,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        # here guidance_scale is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            max_length = text_input.input_ids.shape[-1]\n",
    "            uncond_input = self.tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
    "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
    "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502 and should be between [0, 1]\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        latents = init_latents\n",
    "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
    "        for i, t in tqdm(enumerate(self.scheduler.timesteps[t_start:])):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
    "\n",
    "        # scale and decode the image latents with vae\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = self.vae.decode(latents).sample\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "        # run NSFW safety checker\n",
    "        safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
    "        image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
    "\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        return {\"images\": image, \"nsfw_content_detected\": has_nsfw_concept}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. **Text To Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 **参数配置**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "文字提示 - 模型将根据文字提示的内容生成相应的图片, 一般可分三个部分\n",
    "1. 主体内容(熊猫、武士或高山等);\n",
    "2. 抽象风格样式(抽象形容加具体指代)如流派加艺术家([写实的、Portrait]、[油画、Edgar Degas]、[铅笔画、Rembrandt]);\n",
    "3. 补充润色(4k, washed colors, sharp, beautiful, post processing, ambient lighting, epic composition)\n",
    "\"\"\"\n",
    "prompt_dict = {\n",
    "    \"0000\": \"A photo of an astronaut riding a horse on mars\",\n",
    "    \"0001\": \"Digital art of portrait of a woman, holding pencil, inspired, head-and-shoulders shot, white background, cute pixar character\",\n",
    "    \"0002\": \"Digital art of a man looking upwards, eyes wide inwonder, awestruck, in the style of Pixar, Up, character, white background\",\n",
    "    \"0003\": \"The starry sky painting\",\n",
    "    \"0004\": \"Donald Trump wears a panda headgear\",\n",
    "    \"0005\": \"A painting of a fox sitting in a field at sunrise in the style of Claude Monet\",\n",
    "    \"0006\": \"Dreams flowers and maidens\",\n",
    "    \"0007\": \"Teddy bears, working on new AI research, on the moon in the 1980s\",\n",
    "    \"0008\": \"An astronaut, lounging in a tropical resort in space, as pixel art\",\n",
    "    \"0009\": \"The whale was flying in the air, and below was a volcano and a snow-capped mountain\",\n",
    "    \"0010\": \"A beautiful painting, Prince Nezha's Triumph fighting Dragon King's son, colourful clouds, The waves rushed into the sky with the fire, amber&yellow lights pours on the sea, sunset\",\n",
    "    \"0011\": \"Robot, looking at the clouds hanging in the distance, solemn expression, strange background\",\n",
    "    \"0012\": \"An emerald river, the left bank of the river is volcanoes and scorched earth, the right bank of the river is snow-capped mountains and grasslands, the sun is submerged in the clouds, a few rays of white light sprinkled on the water, matte painting trending on artstation HQ\",\n",
    "    \"0013\": \"A dream of a distant galaxy, by Caspar David Friedrich, matte painting trending on artstation HQ\",\n",
    "    \"0014\": \"Product photography framing. digital paint krita render of a small square fantasy vacuum - tube motherboard made and powered by crystalline circuitry. trending on artstation. artificer's lab bg. premium print by angus mckie and james gurney\",\n",
    "}\n",
    "prompt = prompt_dict[\"0014\"]\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"  # 模型版本 - 一般可设置为 CompVis/stable-diffusion-v1-4 或 runwayml/stable-diffusion-v1-5\n",
    "device = \"cpu\"  # 硬件类型 - 一般可设置为 cpu 或 cuda, 其中 cuda 即 nvidi gpu 的运算驱动平台\n",
    "fp_mode = \"fp32\"  # 浮点数运算精度 - fp32 即 float32 单精度浮点数, fp16 即 float16 半精度浮点数, 一般精度越高计算负载越高生成效果越好\n",
    "sd_mode = \"DDIM\"  # 调度器 - 定义了用于在训练期间向模型添加噪声的噪声计划, 根据先前的噪声表示和预测的噪声残差计算预测的去噪图像表示 \n",
    "sample_num = 1  # 模型推理的次数 - 即循环执行当前模型的次数\n",
    "batch = 1  # 模型并行推理的批量 - 使用多批次数将同时生成多张图像, 2 意味着一次推理将生成 2 张图像, 内存的需求也会较 1 增加\n",
    "height = 512  # 生成图像的高度 - 需要是 8 的倍数(低于 512 将降低图像质量, 与宽同时超过 512 将丧失全局连贯性)\n",
    "width = 512  # 生成图像的宽度 - 需要是 8 的倍数(低于 512 将降低图像质量, 与高同时超过 512 将丧失全局连贯性)\n",
    "num_inference_steps = 50  # 每次模型推理的步骤数 - 一般步骤越大生成的图像质量越高, 建议值 50\n",
    "guidance_scale = 7  # 无分类指导因子 - 能让生成图像匹配文字提示, 稳定扩散, 取值范围 0～20, 过高会牺牲图像质量或多样性, 建议值 7～8.5\n",
    "generator = torch.Generator(device=device).manual_seed(3939590921)  # 随机种子 - 使得紧接着的随机数固定, 如果其他条件不改变, 使用具有相同种子的生成器将得到相同的图像输出, 因此当生成了一张好的图像时可以记录随机种子, 然后微调文字提示\n",
    "save_path = \"./results\"  # 图像保存目录 - 相对地址./path表示在和当前程序同级别的目录path下保存, 也可使用绝对地址"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 **载入模型**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if fp_mode == \"fp32\":\n",
    "    if not os.path.isdir(model_id):\n",
    "        model_id = f\"{model_id}\"\n",
    "    print(f\"从{model_id}加载精度为{fp_mode}的权重，驱动模式为{device}\")\n",
    "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id,\n",
    "                                                   scheduler=scheduler,\n",
    "                                                   torch_dtype=torch.float32,\n",
    "                                                   use_auth_token=True).to(device)\n",
    "\n",
    "elif fp_mode == \"fp16\":\n",
    "    if not os.path.isdir(model_id):\n",
    "        model_id = f\"{model_id}\"\n",
    "    print(f\"从{model_id}加载精度为{fp_mode}的权重，驱动模式为{device}\")\n",
    "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id,\n",
    "                                                   revision=\"fp16\",\n",
    "                                                   scheduler=scheduler,\n",
    "                                                   torch_dtype=torch.float16,\n",
    "                                                   use_auth_token=True).to(device)\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "else:\n",
    "    print(\"Current fp_mode only support fp32 or fp16\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 **图像生成、保存和展示**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "image_list = []\n",
    "for idx in range(sample_num):\n",
    "    print(f\"正在生成第{idx+1}批图像，共{batch}张\")\n",
    "    with autocast(\"cuda\"):\n",
    "        data = pipe([prompt]*batch,\n",
    "                    height=height, width=width,\n",
    "                    num_inference_steps=num_inference_steps, guidance_scale=guidance_scale,\n",
    "                    generator=generator)\n",
    "    image = data[\"images\"]\n",
    "    save_image(image, save_path, prompt)\n",
    "    image_list.append(image_grid(image, x=batch, y=batch))\n",
    "show_image(image_list, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. **Text With Image To Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 **参数配置**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt_dict = {\n",
    "    \"0001\": \"a fantasy landscape, trending on artstation\",\n",
    "    }\n",
    "prompt = prompt_dict[\"0001\"]\n",
    "image_url = \"https://img-blog.csdnimg.cn/11fc2a2b3aa1471b9bced71ee0ddf507.jpeg\"  # 图像地址 - 网络图像链接或本地图像路径\n",
    "strength = 0.75  # 调整强度 - 取值范围 (0, 1) 代表文字提示对原图的修改的程度\n",
    "\n",
    "model_id = \"v1-4\"\n",
    "device = \"cpu\"\n",
    "fp_mode = \"fp32\"\n",
    "sd_mode = \"DDIM\" \n",
    "sample_num = 10\n",
    "batch = 1\n",
    "height= 512\n",
    "width= 512 \n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5 \n",
    "generator = torch.Generator(device=device).manual_seed(51)\n",
    "save_path = \"./results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 导入图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "init_image = []\n",
    "try:\n",
    "    init_image = Image.open(BytesIO(requests.get(image_url).content)).convert(\"RGB\")  # 导入网络图片\n",
    "except:\n",
    "    init_image = Image.open(image_url).convert(\"RGB\")  # 导入本地图片\n",
    "finally:\n",
    "    if not init_image:\n",
    "        print(\"图片未被成功导入, 请检查图像地址是否正确\")\n",
    "\n",
    "init_image = init_image.resize((768, 512))\n",
    "show_image([init_image], prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 **模型载入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if fp_mode == \"fp32\":\n",
    "    if not os.path.isdir(model_id):\n",
    "        model_id = f\"{model_id}\"\n",
    "    print(f\"从{model_id}加载精度为{fp_mode}的权重，驱动模式为{device}\")\n",
    "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    pipe = StableDiffusionImgToImgPipeline.from_pretrained(model_id,\n",
    "                                                           scheduler=scheduler,\n",
    "                                                           torch_dtype=torch.float32,\n",
    "                                                           use_auth_token=True).to(device)\n",
    "\n",
    "elif fp_mode == \"fp16\":\n",
    "    if not os.path.isdir(model_id):\n",
    "        model_id = f\"{model_id}\"\n",
    "    print(f\"从{model_id}加载精度为{fp_mode}的权重，驱动模式为{device}\")\n",
    "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    pipe = StableDiffusionImgToImgPipeline.from_pretrained(model_id,\n",
    "                                                           revision=\"fp16\",\n",
    "                                                           scheduler=scheduler,\n",
    "                                                           torch_dtype=torch.float16,\n",
    "                                                           use_auth_token=True).to(device)\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "else:\n",
    "    print(\"Current fp_mode only support fp32 or fp16\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 **图像生成、保存和展示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "image_list = []\n",
    "for idx in range(sample_num):\n",
    "    print(f\"正在生成第{idx+1}批图像，共{batch}张\")\n",
    "    with autocast(\"cuda\"):\n",
    "        data = pipe([prompt]*batch,\n",
    "                    height=height, width=width,\n",
    "                    num_inference_steps=num_inference_steps, guidance_scale=guidance_scale,\n",
    "                    generator=generator)\n",
    "    image = data[\"images\"]\n",
    "    save_image(image, save_path, prompt)\n",
    "    image_list.append(image_grid(image, x=batch, y=batch))\n",
    "show_image(image_list, prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ***Apple Stabled Diffusion***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 PyTorch 模型转换为 Core ML 模型\n",
    "```shell\n",
    "# use python pipeline\n",
    "python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-text-encoder --convert-vae-decoder --convert-safety-checker -o \"./weights/v1.4\" --model-version \"CompVis/stable-diffusion-v1-4\" --attention-implementation \"ORIGINAL\"\n",
    "\n",
    "# use swift pipeline\n",
    "python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-text-encoder --convert-vae-decoder --convert-safety-checker -o \"./weights/v1.4\" --model-version \"CompVis/stable-diffusion-v1-4\" --attention-implementation \"ORIGINAL\" --chunk-unet --bundle-resources-for-swift-cli\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Performance Benchmark\n",
    "\n",
    "Standard CompVis/stable-diffusion-v1-4 Benchmark\n",
    "\n",
    "| Device                             | `--compute-unit` | `--attention-implementation` | Latency (seconds) |\n",
    "| ---------------------------------- | ---------------- | ---------------------------- | ----------------- |\n",
    "| Mac Studio (M1 Ultra, 64-core GPU) | `CPU_AND_GPU`    | `ORIGINAL`                   | 9                 |\n",
    "| Mac Studio (M1 Ultra, 48-core GPU) | `CPU_AND_GPU`    | `ORIGINAL`                   | 13                |\n",
    "| MacBook Pro (M1 Max, 32-core GPU)  | `CPU_AND_GPU`    | `ORIGINAL`                   | 18                |\n",
    "| MacBook Pro (M1 Max, 24-core GPU)  | `CPU_AND_GPU`    | `ORIGINAL`                   | 20                |\n",
    "| MacBook Pro (M1 Pro, 16-core GPU)  | `ALL`            | `SPLIT_EINSUM (default)`     | 26                |\n",
    "| MacBook Pro (M2)                   | `CPU_AND_NE`     | `SPLIT_EINSUM (default)`     | 23                |\n",
    "| MacBook Pro (M1)                   | `CPU_AND_NE`     | `SPLIT_EINSUM (default)`     | 35                |\n",
    "| iPad Pro (5th gen, M1)             | `CPU_AND_NE`     | `SPLIT_EINSUM (default)`     | 38                |\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Image Generation Example\n",
    "```shell\n",
    "# pytho\n",
    "python -m python_coreml_stable_diffusion.pipeline --prompt \"a photo of an astronaut riding a horse on mars\" -i \"./weights/v1.4/\" --seed 93 --compute-unit CPU_AND_GPU -o \"StableDiffusion/results/apple/\"\n",
    "\n",
    "# swift\n",
    "swift run StableDiffusionSample \"a photo of an astronaut riding a horse on mars\" --resource-path \"./weights/v1.4/Resources/\" --seed 93 --output-path \"StableDiffusion/results/apple/\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22657685f323f6adad5908dc41c3487a967b5aa9c39897c5cf3f8a30d6cfa27a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
